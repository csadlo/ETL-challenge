{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Glenda and Chris' Awesome Code!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#added by glenda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependencies and Setup\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import time\n",
    "from scipy.stats import linregress\n",
    "import country_converter as coco\n",
    "from sqlalchemy import create_engine\n",
    "import gmaps\n",
    "\n",
    "# Import API key\n",
    "from api_keys import weather_api_key\n",
    "from api_keys import google_key\n",
    "from config import username\n",
    "from config import password\n",
    "\n",
    "\n",
    "gmaps.configure(api_key = google_key)\n",
    "\n",
    "\n",
    "# Incorporated citipy to determine city based on latitude and longitude\n",
    "from citipy import citipy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Store CSV files into dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXTRACTING data from NOAA's HURDAT2 available at this website:\n",
    "# https://www.kaggle.com/noaa/hurricane-database\n",
    "\n",
    "csv_file = \"Resources/atlantic.csv\"\n",
    "atlantic_data_df = pd.read_csv(csv_file)\n",
    "atlantic_data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRANSFORMING the Atlantic Ocean data by dropping columns that were largely unused\n",
    "\n",
    "new_atlantic_data_df = atlantic_data_df[[\"ID\",\"Name\",\"Date\",\"Time\",\"Status\",\"Latitude\",\"Longitude\",\"Maximum Wind\",\"Minimum Pressure\"]]\n",
    "new_atlantic_data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXTRACTING data from NOAA's HURDAT2 available at this website:\n",
    "# https://www.kaggle.com/noaa/hurricane-database\n",
    "\n",
    "csv_file = \"Resources/pacific.csv\"\n",
    "pacific_data_df = pd.read_csv(csv_file)\n",
    "pacific_data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRANSFORMING the Pacific Ocean data by dropping columns that were largely unused\n",
    "\n",
    "new_pacific_data_df = pacific_data_df[[\"ID\",\"Name\",\"Date\",\"Time\",\"Status\",\"Latitude\",\"Longitude\",\"Maximum Wind\",\"Minimum Pressure\"]]\n",
    "new_pacific_data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRANSFORMING a combination of Atlantic Ocean and Pacific Ocean data into a single dataframe\n",
    "\n",
    "atlantic_and_pacific_data = pd.concat([new_atlantic_data_df, new_pacific_data_df])\n",
    "atlantic_and_pacific_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONFIRMING the datatypes that were Extracted\n",
    "\n",
    "atlantic_and_pacific_data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRANSFORMING by filtering the data from the CSV files to shrink the data to process\n",
    "\n",
    "# Exclude any data from hurricane season prior to the year 2000  (Date >= year 2000)\n",
    "atlantic_and_pacific_2000 = atlantic_and_pacific_data.loc[atlantic_and_pacific_data[\"Date\"] >= 20000000, :]\n",
    "\n",
    "clean_atlantic_and_pacific_df = atlantic_and_pacific_2000\n",
    "\n",
    "# Exclude any data from hurricane season that was less than 60 mph (Maximum Wind >= 60)\n",
    "clean_atlantic_and_pacific_df = atlantic_and_pacific_2000.loc[atlantic_and_pacific_2000[\"Maximum Wind\"] >= 60, :]\n",
    "clean_atlantic_and_pacific_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRANSFORM the data by converting from a user-friendly format of latitude and longitude (eg. 51N, 101W)\n",
    "# to one that was more commonly used with APIs (e.g. 51, -101)\n",
    "\n",
    "# convert latitude and longitude values to decimal format\n",
    "def cardinal2negative(s):\n",
    "    degrees = float(s[:-1])\n",
    "    cardinal = s[len(s)-1:]\n",
    "    \n",
    "    if cardinal in ('s','S','w','W'):\n",
    "        degrees *= -1\n",
    "\n",
    "    return degrees\n",
    "\n",
    "# Perform this step to skip data copying warning from python\n",
    "clean_atlantic_and_pacific_df = clean_atlantic_and_pacific_df.copy()\n",
    "lat_series = clean_atlantic_and_pacific_df[\"Latitude\"].copy().apply(cardinal2negative)\n",
    "lng_series = clean_atlantic_and_pacific_df['Longitude'].copy().apply(cardinal2negative)\n",
    "\n",
    "clean_atlantic_and_pacific_df[\"New Latitude\"] = lat_series\n",
    "clean_atlantic_and_pacific_df['New Longitude'] = lng_series\n",
    "\n",
    "# TRANSFORM the data by removing the leading whitespace in-front of the\n",
    "# hurricane's Name and Status left-over from CSV\n",
    "clean_atlantic_and_pacific_df[\"Name\"] = clean_atlantic_and_pacific_df[\"Name\"].str.strip()\n",
    "clean_atlantic_and_pacific_df[\"Status\"] = clean_atlantic_and_pacific_df[\"Status\"].str.strip()\n",
    "clean_atlantic_and_pacific_df\n",
    "\n",
    "clean_atlantic_and_pacific_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################################\n",
    "# THIS STEP WILL TAKE A FEW MINUTES TO RUN. PLEASE BE PATIENT (coco is slow) #\n",
    "##############################################################################\n",
    "\n",
    "# TRANSFORM the data by using the citipy API to determine the nearest city \n",
    "# for each lat lng pair and add it to the dataframe\n",
    "\n",
    "unique_cities_list = []\n",
    "city_list = []\n",
    "country_list = []\n",
    "\n",
    "# Create a set of latitude and longitude combinations to process over in a simple for-loop\n",
    "lats = clean_atlantic_and_pacific_df[\"New Latitude\"]\n",
    "lngs = clean_atlantic_and_pacific_df[\"New Longitude\"]\n",
    "lat_lngs = zip(lats, lngs)\n",
    "\n",
    "# EXTRACT from the citipy API to identify nearest city for each lat, lng combination\n",
    "for lat_lng in lat_lngs:\n",
    "    city = citipy.nearest_city(lat_lng[0], lat_lng[1]).city_name\n",
    "    country = citipy.nearest_city(lat_lng[0], lat_lng[1]).country_code\n",
    "    \n",
    "    # TRANSFORM the data by uppercasing cities and converting country codes to country names\n",
    "    city = city.title()\n",
    "    country = coco.convert(names=country, to='name_short')\n",
    "\n",
    "    city_list.append(city)\n",
    "    country_list.append(country)\n",
    "\n",
    "    if (country == \"not found\"):\n",
    "        continue ### We want to skip this location if we couldn't convert the country code\n",
    "\n",
    "    city_country_pair = f\"{city}, {country}\"\n",
    "    \n",
    "    # If the city is unique, then add it to our unique cities list\n",
    "    if city_country_pair not in unique_cities_list:\n",
    "        unique_cities_list.append(city_country_pair)\n",
    "\n",
    "\n",
    "# Add the city and country pair to the dataframe here:\n",
    "clean_atlantic_and_pacific_df[\"City\"] = city_list\n",
    "clean_atlantic_and_pacific_df[\"Country\"] = country_list\n",
    "\n",
    "### Warning for the Netherland's Antilles island's country code \"AN\" for not being in citipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean_atlantic_and_pacific_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRANSFORM the data by filtering out any location where the country code could not be converted\n",
    "clean_atlantic_and_pacific_df = clean_atlantic_and_pacific_df[clean_atlantic_and_pacific_df[\"Country\"] != \"not found\"]\n",
    "\n",
    "# Print the city count to confirm sufficient count\n",
    "len(unique_cities_list)\n",
    "unique_cities_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean_atlantic_and_pacific_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_atlantic_and_pacific_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD - Perform the loading of the data into the sql server here:\n",
    "rds_connection_string = f'{username}:{password}@localhost:5432/hurricanes'\n",
    "engine = create_engine(f'postgresql://{rds_connection_string}')\n",
    "\n",
    "# Confirm the table name in the database:\n",
    "engine.table_names()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names = clean_atlantic_and_pacific_df.columns\n",
    "column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRANSFORM the data by converting the column names to something more palatable by postgres\n",
    "\n",
    "# Consrtruct an empty list to be populated soon\n",
    "sql_column_names = []\n",
    "\n",
    "# Make a deep copy of the hurricane dataframe\n",
    "to_sql_df = clean_atlantic_and_pacific_df.copy()\n",
    "\n",
    "# Loop over all the column names\n",
    "for name in column_names:\n",
    "    \n",
    "    # And append them to a list of lowercased column names\n",
    "    sql_column_names.append(name.lower())\n",
    "    \n",
    "# Set all the column names in the deep-copy dataframe to their lowercase equivalents\n",
    "to_sql_df.columns = sql_column_names\n",
    "\n",
    "# TRANSFORM\n",
    "# Rename the columns that have spaces in the middle to use underscores instead\n",
    "to_sql_df.rename(columns={\"date\":\"date_stamp\", \n",
    "                          \"time\":\"time_stamp\", \n",
    "                          \"maximum wind\":\"max_wind\",\n",
    "                          \"minimum pressure\":\"min_pressure\",\n",
    "                          \"new latitude\":\"new_latitude\",\n",
    "                          \"new longitude\":\"new_longitude\"} , inplace=True)\n",
    "\n",
    "to_sql_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_sql_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD the data INTO the SQL server database\n",
    "to_sql_df.to_sql(name='hurricanes', con=engine, if_exists='replace', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD/EXTRACT the data FROM the SQL database\n",
    "katrina_from_sql_df = pd.read_sql_query(\"select * from hurricanes where name = 'KATRINA'\", con=engine)\n",
    "coords = katrina_from_sql_df[[\"new_latitude\", \"new_longitude\"]]\n",
    "\n",
    "# EXTRACT Google Maps data\n",
    "fig = gmaps.figure(zoom_level=5, center=(27.5,-85))\n",
    "\n",
    "#heat_layer = gmaps.heatmap_layer(coords, dissipating=False, max_intensity=100, point_radius=3)\n",
    "#fig.add_layer(heat_layer)\n",
    "\n",
    "# TRANSFORM the EXTRACTED data into a Google Maps symbol layer\n",
    "hurricane_katrina_layer = gmaps.symbol_layer(\n",
    "    coords, fill_color='red', stroke_color='red', scale=2\n",
    ")\n",
    "fig.add_layer(hurricane_katrina_layer)\n",
    "fig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD/EXTRACT the data FROM the SQL database\n",
    "all_hurricanes_from_sql_df = pd.read_sql_query('select * from hurricanes where max_wind >= 115', con=engine)\n",
    "coords = all_hurricanes_from_sql_df[[\"new_latitude\", \"new_longitude\"]]\n",
    "\n",
    "# EXTRACT Google Maps data\n",
    "fig = gmaps.figure(zoom_level=3, center=(25,-75))\n",
    "\n",
    "# TRANSFORM the EXTRACTED data into a Google Maps symbol layer\n",
    "all_hurricanes_layer = gmaps.symbol_layer(\n",
    "    coords, fill_color='green', stroke_color='green', scale=2\n",
    ")\n",
    "fig.add_layer(all_hurricanes_layer)\n",
    "fig.add_layer(hurricane_katrina_layer)\n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD/EXTRACT the data FROM the SQL database\n",
    "all_hurricanes_from_sql_df = pd.read_sql_query('select * from hurricanes where max_wind >= 115', con=engine)\n",
    "coords = all_hurricanes_from_sql_df[[\"new_latitude\", \"new_longitude\"]]\n",
    "winds = all_hurricanes_from_sql_df[\"max_wind\"]\n",
    "\n",
    "winds.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXTRACT Google Maps data\n",
    "fig = gmaps.figure(zoom_level=3, center=(25,-75))\n",
    "\n",
    "# TRANSFORM the EXTRACTED data into a Google Maps heatmap layer\n",
    "heat_layer = gmaps.heatmap_layer(coords, weights=winds, dissipating=True, max_intensity=150, point_radius=7)\n",
    "\n",
    "fig.add_layer(heat_layer)\n",
    "fig.add_layer(hurricane_katrina_layer)\n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
